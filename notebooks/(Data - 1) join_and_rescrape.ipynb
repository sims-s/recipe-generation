{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import uuid\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.utils import fast_string_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Kaggle and Recipes 1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a website from a url in onem recipes dataset\n",
    "def get_website(url):\n",
    "    return url[(max(url.find('//') + 2, url.find('www.') + 4)):url.find('.com')].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshhold = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 9999)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Food: https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions\n",
    "* First, make the data look nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_datapath = '../data/kaggle_food/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_recipes = pd.read_csv(kaggle_datapath + 'RAW_recipes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_reviews = pd.read_csv(kaggle_datapath + 'RAW_interactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_recipes['ingredients'] = kaggle_recipes['ingredients'].progress_apply(eval)\n",
    "kaggle_recipes['steps'] = kaggle_recipes['steps'].progress_apply(eval)\n",
    "kaggle_recipes['tags'] = kaggle_recipes['tags'].progress_apply(eval)\n",
    "kaggle_recipes['nutrition'] = kaggle_recipes['nutrition'].progress_apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pbar_list(elems):\n",
    "    pbar.update(1)\n",
    "    return list(elems)\n",
    "\n",
    "pbar = tqdm(total = 2*kaggle_reviews['recipe_id'].nunique(), leave=False)\n",
    "kaggle_reviews = kaggle_reviews.groupby('recipe_id').agg({'rating' : pbar_list, 'review' : pbar_list})\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df = kaggle_recipes.merge(kaggle_reviews, left_on='id', right_on='recipe_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand nutrition field based on this info: https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions/discussion/121778\n",
    "# entries are: [calories (#), total fat (PDV), sugar (PDV) , sodium (PDV) , protein (PDV) , saturated fat (PDV) , and carbohydrates (PDV)]\n",
    "# pdv = percent daily value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df['calories'] = kaggle_df['nutrition'].apply(lambda x: x[0]).astype(float)\n",
    "kaggle_df['total_fat_pdv'] = kaggle_df['nutrition'].apply(lambda x: x[1] / 100).astype(float)\n",
    "kaggle_df['sugar_pdv'] = kaggle_df['nutrition'].apply(lambda x: x[2] / 100).astype(float)\n",
    "kaggle_df['sodium_pdv'] = kaggle_df['nutrition'].apply(lambda x: x[3] / 100).astype(float)\n",
    "kaggle_df['protien_pdv'] = kaggle_df['nutrition'].apply(lambda x: x[4] / 100).astype(float)\n",
    "kaggle_df['saturated_fat_pdv'] = kaggle_df['nutrition'].apply(lambda x: x[5] / 100).astype(float)\n",
    "kaggle_df['carb_pdv'] = kaggle_df['nutrition'].apply(lambda x: x[6] / 100).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df['time'] = kaggle_df['minutes'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(kaggle_df['n_ingredients']==kaggle_df['ingredients'].apply(len)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['id', 'contributor_id', 'submitted', 'submitted', 'nutrition', 'n_steps', 'n_ingredients', 'minutes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kaggle_df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipes 1M: http://pic2recipe.csail.mit.edu/\n",
    "* Download From: http://data.csail.mit.edu/im2recipe/recipe1M_layers.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe1m_path = '../data/recipe_1m/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onem_recipes = load_json(recipe1m_path + 'layer1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(onem_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_raw_from_id(id):\n",
    "    for r in onem_recipes:\n",
    "        if r['id']==id:\n",
    "            pprint.pprint(r)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_raw_from_id(\"000018c8a5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onem_processed = []\n",
    "for recipe in tqdm(onem_recipes):\n",
    "    ingredients = [i['text'] for i in recipe['ingredients']]\n",
    "    instructions = [i['text'] for i in recipe['instructions']]\n",
    "    onem_processed.append({\n",
    "        'title': recipe['title'],\n",
    "        'ingredients' : ingredients,\n",
    "        'instructions' : instructions,\n",
    "        'url': recipe['url'],\n",
    "        'id' : recipe['id']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onem_df = pd.DataFrame.from_dict(onem_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onem_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "onem_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "onem_df['source'] = onem_df['url'].progress_apply(get_website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onem_df['title_lower'] = onem_df['title'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1M recipes has duplication issues... lets remove them...\n",
    "* Assumption I haven't verified: If names are different, then can't be a duplicate\n",
    "    * Maybe run into issues with some extra punctuation in titles so some duplicates aren't caught\n",
    "    * But like groupby makes things sooooo easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lots of poentital duplicates\n",
    "(onem_df['title_lower'].value_counts() > 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onem_df[onem_df['title_lower']==\"a good easy garlic chicken\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "onem_df[onem_df['title_lower']=='almond chocolate coffee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "onem_df[onem_df['title_lower']=='apple pepper jelly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(grouped_df):\n",
    "    if grouped_df.shape[0]==1:\n",
    "        deduped_rows.append(grouped_df.iloc[0].tolist())\n",
    "    else:\n",
    "        # used: either appended to the uniuqe list OR not including b/c its a duplicate\n",
    "        used = np.zeros(grouped_df.shape[0]).astype(bool)\n",
    "        steps = [' '.join(s) for s in grouped_df['instructions'].tolist()]\n",
    "        kept = []\n",
    "        for i in range(grouped_df.shape[0]):\n",
    "            if used[i]:\n",
    "                continue\n",
    "            kept.append(i)\n",
    "            used[i] = True\n",
    "            similarities = [-1] * (i+1) + []\n",
    "            for j in range(i+1, grouped_df.shape[0]):\n",
    "                similarities.append(fast_string_sim(steps[i], steps[j]))\n",
    "            similarities = np.array(similarities)\n",
    "            dup_indexer = similarities > score_threshhold\n",
    "            used[dup_indexer] = True\n",
    "            deduped_rows.append(grouped_df.iloc[i].tolist())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = onem_df.groupby('title_lower').progress_apply(dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onem_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onem_df = pd.DataFrame.from_records(deduped_rows, columns = list(onem_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's not 1 million. THEY LIED TO ME :p\n",
    "onem_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lots of poentital duplicates\n",
    "(onem_df['title_lower'].value_counts() > 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked on those three cases at least! :)\n",
    "display(onem_df[onem_df['title_lower']==\"a good easy garlic chicken\"])\n",
    "display(onem_df[onem_df['title_lower']=='almond chocolate coffee'])\n",
    "display(onem_df[onem_df['title_lower']=='apple pepper jelly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join the datasets!\n",
    "### Join Logic:\n",
    "* There are multiple entries with the same names in both datasets, so we can't use name to join :(\n",
    "* Looks like we can use name + instructions though\n",
    "    * For each row in Kaggle dataset, get # of same named ones\n",
    "    * If there's just a single one, then they're a match\n",
    "    * If there are multiple in food, then compute the % word overlap in the instructions to pick the best match. Also compare length so you don't just have a really really long recipe coincidentally contain all the right words. I'm guessing this is faster than doing an edit distance type thingy, but maybe that'd be better. IDK for sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some dictionary temporary vars s.t. things move faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruction_agg(instructions):\n",
    "    pbar.update(1)\n",
    "    return [' '.join(i).lower() for i in list(instructions)]\n",
    "\n",
    "pbar = tqdm(total = onem_df['title_lower'].nunique(), leave=False)\n",
    "name_to_instructions = onem_df.groupby('title_lower').agg({'instructions' : instruction_agg, 'id' : list}).to_dict(orient='index')\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in name_to_instructions.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onem_id_to_uid = defaultdict(lambda: -1)\n",
    "kaggle_uid_list = []\n",
    "# Record the similarity history to pick a cutoff\n",
    "sim_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(total = kaggle_df.shape[0], leave=False)\n",
    "for name, instruction in zip(kaggle_df['name'].tolist(), kaggle_df['steps'].tolist()):\n",
    "    this_uid = str(uuid.uuid4())\n",
    "    kaggle_uid_list.append(this_uid)\n",
    "    \n",
    "    if name in name_to_instructions:\n",
    "        possible_instructions = name_to_instructions[name]\n",
    "        instruction = ' '.join(instruction)\n",
    "\n",
    "        similarities = []\n",
    "        for potential_match in possible_instructions['instructions']:\n",
    "            similarities.append(fast_string_sim(instruction, potential_match))\n",
    "            \n",
    "        match_idx = -1\n",
    "        curr_max_val = -1\n",
    "        for i, s in enumerate(similarities):\n",
    "            if s > score_threshhold and s > curr_max_val:\n",
    "                match_idx = i\n",
    "                curr_max_val = s\n",
    "        if match_idx >=0:\n",
    "            onem_id_to_uid[possible_instructions['id'][match_idx]] = this_uid\n",
    "            \n",
    "        sim_hist.append(similarities)\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(itertools.chain.from_iterable(sim_hist))).hist()\n",
    "plt.title('Similarities on potential joins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df['uid'] = kaggle_uid_list\n",
    "onem_df['uid'] = onem_df['id'].map(onem_id_to_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join %\n",
    "kaggle_df.shape[0], (onem_df['uid']!=-1).sum(), onem_df[onem_df['source']=='food']['title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onem_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df.drop(['name', 'steps', 'description', 'ingredients'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onem_df.shape, kaggle_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = onem_df.merge(kaggle_df, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.drop('uid', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Before preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get an estimate on the number of times fraction bars are omitted (e.g. 1/2 --> 12)\n",
    "* I'd like the recipes to be accurate, so need to clean them up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all URLS seem to be reasonable, so if needed, can just scrape url and check\n",
    "merged['url'].isnull().mean(), merged['url'].apply(lambda x: len(x) < 5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_twelve(ingredient_list):\n",
    "    return any(['12' in x for x in ingredient_list])\n",
    "def has_one_half(ingredient_list):\n",
    "    return any(['1/2' in x for x in ingredient_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['ingredients_twelve'] = merged['ingredients'].apply(has_twelve)\n",
    "merged['ingredients_half'] = merged['ingredients'].apply(has_one_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged.groupby('source').agg({'ingredients_twelve' : {'mean', 'sum'}}).sort_values(('ingredients_twelve', 'mean'), ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seems to be a frequent issue in food, but not everywhere else\n",
    "\n",
    "| Source      | Description |\n",
    "| ----------- | ----------- |\n",
    "| Food      | Lots of issues       |\n",
    "| online-cookbook   | Reason lots of twelves show up because liquid ingredients include amount in ml. Often see (125 ml, e.g.)|\n",
    "| cookstr | Generally 12s make sense|\n",
    "| foodnetwork| Generally 12s make sens|\n",
    "| tastykitchen| Generally 12s make sense|  \n",
    "  \n",
    "     \n",
    "* Just sampling the \"worst\" ones interms of 1/2s, looks like food.com ones are the worst by a decent amount\n",
    "* So those are the only ones that I'm going to rescrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "def process_ingredient_list(ingredient_list):\n",
    "    for i in range(len(ingredient_list)):\n",
    "        ingredient_list[i] = re.sub(' +', ' ', ingredient_list[i])\n",
    "        ingredient_list[i] = ingredient_list[i].strip()\n",
    "        ingredient_list[i] = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', ingredient_list[i])\n",
    "    return ingredient_list\n",
    "\n",
    "def update_ingredients(row):\n",
    "    source = row['source']\n",
    "    row['my_ingredients'] = []\n",
    "    if not row['source']=='food':\n",
    "        return row\n",
    "    url = row['url']\n",
    "    time.sleep(np.random.uniform(.25, .75))\n",
    "    response = requests.get(url)\n",
    "    ingredient_list = []\n",
    "    try:\n",
    "        if not response.status_code >= 400:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            info_dict = soup.find_all(type='application/ld+json')[0].text\n",
    "            if not 'recipeIngredient' in info_dict:\n",
    "                return row\n",
    "            info_dict = eval(info_dict)\n",
    "            ingredient_list = info_dict['recipeIngredient']\n",
    "        else:\n",
    "            print('%^d @ %s'%(response.status_code, url))\n",
    "            \n",
    "        ingredient_list = process_ingredient_list(ingredient_list)\n",
    "    except (ValueError, AttributeError, TypeError):\n",
    "        return row\n",
    "    except requests.ConnectionError:\n",
    "        print('Connection Reset, Sleeping for a while...')\n",
    "        time.sleep(20*60)\n",
    "        return update_ingredients(row)\n",
    "\n",
    "    row['my_ingredients'] = ingredient_list\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break up into lots of chunks b/c hits their website a bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.sort_values('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_indicies = np.arange(317)*3125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../data/interim/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(chunk_indicies)-1):\n",
    "    print('Chunk %d'%i)\n",
    "    sub_df = merged.iloc[chunk_indicies[i]:chunk_indicies[i+1]]\n",
    "    sub_df = sub_df.progress_apply(update_ingredients, axis=1)\n",
    "    sub_df.to_pickle(save_path + 'after_scraping_chunk_%d.pkl'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
